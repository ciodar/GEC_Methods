{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk.translate.chrf_score\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#plt.switch_backend('agg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grammar Error Correction with Transformers\n",
    "======================================================\n",
    "\n",
    "This notebook shows how to train a GEC model based on transformers.\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "- [1. Data Sourcing and Processing](#1)\n",
    "    - Tokenizing and Embedding\n",
    "    - Collation\n",
    "- [2. Seq2Seq Network using Transformer](#2)\n",
    "    - Positional encoding\n",
    "    - Multi-head attention\n",
    "- [3. Model definition](#3)\n",
    "- [4. Training](#4)\n",
    "- [5. Evaluation](#5)\n",
    "- [6. Results](#6)\n",
    "- [7. References](#7)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='1'></a>\n",
    "# 1. Data Sourcing and Processing\n",
    "\n",
    "C4 200M dataset from Google Research is used in this notebook. You can find more information about the C4 200M dataset on Google Research's [BEA 2021 paper](https://aclanthology.org/2021.bea-1.4/) (Stahlberg and Kumar, 2021).\n",
    "\n",
    "The already [processed dataset](https://huggingface.co/datasets/liweili/c4_200m) was extracted from Huggingface in CSV format, then was transformed to HDF5 format for better manageability. The conversion process is detailed in ``utils.py``, and was based on this [notebook](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb).\n",
    "The final version of the dataset is uploaded on [Kaggle](https://www.kaggle.com/datasets/dariocioni/c4200m).\n",
    "\n",
    "A custom class ``Hdf5Dataset`` based on ``torch.utils.data.Dataset`` is developed, which yields a pair of source-target raw sentences.\n",
    "\n",
    "| source                                             | target                                                  |\n",
    "|----------------------------------------------------|---------------------------------------------------------|\n",
    "| Much many brands and sellers still in the market.  | Many brands and sellers still in the market.            |\n",
    "| She likes playing in park and come here every week | She likes playing in the park and comes here every week |\n",
    "\n",
    "To be able to train on a arbitrary subset of a single file, the dataset only reads chunks of length ``num_entries`` if the parameter is specified . In order to capture more examples, a different chunk can be randomly chosen at each epoch by specifying ``randomized=True``"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import pathlib as pl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "random.seed(42)\n",
    "\n",
    "class Hdf5Dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, h5_path, transform=None,num_entries = None,randomized=False):\n",
    "\n",
    "        self.h5f = h5py.File(h5_path, 'r')\n",
    "        self.size = self.h5f['labels'].shape[0]\n",
    "        self.transform = transform\n",
    "        self.randomized = randomized\n",
    "        self.max_index = num_entries if num_entries is not None else self.size\n",
    "        #Chooses an offset for the dataset when using a subset of a Hdf5 file\n",
    "        if randomized:\n",
    "            self.offset = random.choice(range(0,self.size//self.max_index))*self.max_index\n",
    "        else:\n",
    "            self.offset = 0\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index > self.max_index:\n",
    "            raise StopIteration\n",
    "        input = self.h5f['input'][self.offset+index].decode('utf-8')\n",
    "        label = self.h5f['labels'][self.offset+index].decode('utf-8')\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(input)\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.max_index\n",
    "\n",
    "    def reshuffle(self):\n",
    "        if self.randomized:\n",
    "            self.offset = random.choice(range(0,self.size//self.max_index))*self.max_index\n",
    "        else:\n",
    "            print(\"Please set randomized=True\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "SRC_LANGUAGE = 'incorrect'\n",
    "TGT_LANGUAGE = 'correct'\n",
    "MAX_LENGTH = 5000\n",
    "VOCAB_SIZE = 20000\n",
    "N_SAMPLES = 100000\n",
    "\n",
    "# Place-holders\n",
    "token_transform =get_tokenizer('basic_english')\n",
    "vocab_transform = None\n",
    "\n",
    "folder = 'dataset'\n",
    "train_filename = 'train.hf5'\n",
    "valid_filename = 'valid.hf5'\n",
    "test_filename = 'test.hf5'\n",
    "vocab_path = 'vocab/vocab_20K.pth'\n",
    "embedding_path = 'vocab/glove_42B_300d_20K.pth'\n",
    "checkpoint_folder = 'G:\\Il mio Drive\\Colab Notebooks\\GEC_Soft_Masked_BERT\\checkpoints'\n",
    "\n",
    "##COLAB\n",
    "# folder = '/content/drive/MyDrive/Datasets/c4_200m/hdf5'\n",
    "# train_filename = 'C4_200M.hf5-00000-of-00010'\n",
    "# valid_filename = 'C4_200M.hf5-00001-of-00010'\n",
    "# embedding_path = '/content/drive/MyDrive/Colab Notebooks/GEC_Soft_Masked_BERT/vocab/glove_42B_300d_20K.pth'\n",
    "# vocab_path = '/content/drive/MyDrive/Colab Notebooks/GEC_Soft_Masked_BERT/vocab/vocab_20K.pth'\n",
    "# checkpoint_folder = '/content/drive/MyDrive/Colab Notebooks/GEC_Soft_Masked_BERT/checkpoints'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizing and Embedding\n",
    "Data is then tokenized by the standard tokenizer from ``torchtext`` library, which performs basic normalization and splitting by space. Normalization includes\n",
    "- lowercasing\n",
    "- complete some basic text normalization for English words as follows:\n",
    "    add spaces before and after '\\''\n",
    "    remove '\\\"',\n",
    "    add spaces before and after '.'\n",
    "    replace '<br \\/>'with single space\n",
    "    add spaces before and after ','\n",
    "    add spaces before and after '('\n",
    "    add spaces before and after ')'\n",
    "    add spaces before and after '!'\n",
    "    add spaces before and after '?'\n",
    "    replace ';' with single space\n",
    "    replace ':' with single space\n",
    "    replace multiple spaces with single space\n",
    "\n",
    "A vocabulary was produced based on 1M samples of the training dataset, using ``build_vocab`` function inside ``vocab.py``.\n",
    "\n",
    "I then evaluated pre-trained embeddings and confronted with the nn.Embedding, both with embeddings of length 300.\n",
    "- ``GloVe`` Embeddings were trained on Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors)\n",
    "- Pretrained embeddings were aligned with the vocabulary using ``load_pretrained_embs`` function inside ``vocab.py``."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<UNK>', '<PAD>', '<BOS>', '<EOS>']\n",
    "\n",
    "vocab_transform = torch.load(vocab_path)\n",
    "# embeddings = torch.load(embedding_path)\n",
    "embeddings = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collation\n",
    "\n",
    "An iterator over ``Hdf5dataset`` yields a pair of raw strings.\n",
    "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network.\n",
    "Below I defined a collate function that converts batch of raw strings into batch tensors that can be fed directly into the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# def glove_transform(tokens: List[str]):\n",
    "\n",
    "\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    #truncate sequences longer than MAX_LENGTH\n",
    "    if(len(token_ids) > MAX_LENGTH - 2):\n",
    "        token_ids = token_ids[0:MAX_LENGTH -2]\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = sequential_transforms(token_transform,\n",
    "                                       vocab_transform,\n",
    "                                       tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's an example of the encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2],\n",
      "        [ 157],\n",
      "        [   0],\n",
      "        [  13],\n",
      "        [1480],\n",
      "        [  32],\n",
      "        [   3]]) tensor([[   2],\n",
      "        [ 157],\n",
      "        [1185],\n",
      "        [  13],\n",
      "        [1480],\n",
      "        [  32],\n",
      "        [   3]])\n"
     ]
    }
   ],
   "source": [
    "text = ('Data Maining is awesome!','Data Mining is awesome!')\n",
    "src,tgt = collate_fn([text])\n",
    "print(src,tgt)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name='2'></a>\n",
    "# 2. Seq2Seq Network using Transformer\n",
    "\n",
    "Transformer is a Seq2Seq model introduced in [“Attention is all you need”](<https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>) paper for solving machine translation tasks.\n",
    "Below, we will create a Seq2Seq network that uses Transformer. The network consists of three parts:\n",
    "1) The embedding layer. This layer converts tensor of input indices into corresponding tensor of input embeddings.\n",
    "    These embedding are further augmented with ``Positional Encodings``, to provide position information of input tokens to the model.\n",
    "2) The actual [Transformer](<https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html>) model.\n",
    "3) The output of Transformer model is finally passed through a linear layer that give un-normalized probabilities for each token in the target language.\n",
    "\n",
    "### Positional Encoding\n",
    "Differently from RNNs, Transformers don't have a notion of relative or absolute position of the tokens in the input.\n",
    "One solution is to combine the input embeddings with positional embeddings, specific to each position in an input sequence.\n",
    "A solution that is not biased towards the initial positions consists in a combination of sine and cosine functions of different frequencies ([Vaswani et al. ,2017](<https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>))\n",
    "\n",
    "Given an embedding of length $d$, a position in the sequence $pos$ and the $i$-th dimension of the embedding, the position embedding is calculated as\n",
    "\n",
    "$$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d})\\quad,\\quad PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d})$$\n",
    "\n",
    "Dropout is also added to the sums of the embeddings and the positional encodings in both the encoder and decoder.\n",
    "\n",
    "<img src=\"img/pos_enc.png\">\n",
    "\n",
    "### Multi-head attention\n",
    "A single transformer block cannot capture all the different kinds of simultaneous relations among its inputs.\n",
    "To address this problem, Transformers can use multiple self-attention heads, residing in parallel layers and with different parameter sets.\n",
    "Each head $i$ will have a different set of key, query and value matrices $W_i^K,W_i^Q,W_i^V$ and will project into different embeddings for each head.\n",
    "The different embeddings are finally reduced to the original input dimension, using a trainable linear projection $W^O$\n",
    "\n",
    "<img src=\"img/multihead.png\">\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "\n",
    "import math\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 emb_size: int,\n",
    "                 dropout: float,\n",
    "                 maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding: Tensor):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size,embedding_weights=None):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding.weight = torch.nn.Parameter(torch.from_numpy(embedding_weights).float())\n",
    "            # self.embedding.weight.requires_grad =False\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens: Tensor):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "# Seq2Seq Network \n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_encoder_layers: int,\n",
    "                 num_decoder_layers: int,\n",
    "                 emb_size: int,\n",
    "                 nhead: int,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 dim_feedforward: int = 100,\n",
    "                 dropout: float = 0.1,\n",
    "                 embedding_weights = None):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = Transformer(d_model=emb_size,\n",
    "                                       nhead=nhead,\n",
    "                                       num_encoder_layers=num_encoder_layers,\n",
    "                                       num_decoder_layers=num_decoder_layers,\n",
    "                                       dim_feedforward=dim_feedforward,\n",
    "                                       dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size,embedding_weights)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size,embedding_weights)\n",
    "        self.positional_encoding = PositionalEncoding(\n",
    "            emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                trg: Tensor,\n",
    "                src_mask: Tensor,\n",
    "                tgt_mask: Tensor,\n",
    "                src_padding_mask: Tensor,\n",
    "                tgt_padding_mask: Tensor,\n",
    "                memory_key_padding_mask: Tensor):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None, \n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src: Tensor, src_mask: Tensor, src_padding_mask: Tensor = None):\n",
    "        return self.transformer.encoder(self.positional_encoding(\n",
    "                            self.src_tok_emb(src)), src_mask, src_padding_mask)\n",
    "\n",
    "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
    "        return self.transformer.decoder(self.positional_encoding(\n",
    "                          self.tgt_tok_emb(tgt)), memory,\n",
    "                          tgt_mask)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let's define a function that will take care of both.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src, tgt):\n",
    "    src_seq_len = src.shape[0]\n",
    "    tgt_seq_len = tgt.shape[0]\n",
    "\n",
    "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_mask = create_mask(src,tgt)\n",
    "_ = plt.imshow(example_mask[1].cpu(),cmap='hot')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"3\"></a>\n",
    "# 3. Model definition\n",
    "The model is instantiated in the ``Seq2SeqTransformer`` wrapper.\n",
    "I used the cross-entropy loss as the loss function and Adam optimizer for training.\n",
    "The regularization hyperparameters were chosen as in ([Vaswani et al. ,2017](<https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf>))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "VOCAB_SIZE = len(vocab_transform.vocab.itos_)\n",
    "EMB_SIZE = 100\n",
    "NHEAD = 2\n",
    "FFN_HID_DIM = 128\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "\n",
    "#To save and retrieve checkpoints\n",
    "model_name = 'transformer_%dE_%dH_%dF_%d_%d.pt'%(EMB_SIZE,NHEAD,FFN_HID_DIM,NUM_DECODER_LAYERS,NUM_DECODER_LAYERS)\n",
    "\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE, \n",
    "                                 NHEAD, VOCAB_SIZE, VOCAB_SIZE, FFN_HID_DIM,embedding_weights=embeddings)\n",
    "\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "def train_epoch(model, optimizer):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "    train_iter = Hdf5Dataset(pl.Path(folder)/train_filename,num_entries=N_SAMPLES,randomized=True)\n",
    "    train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in tqdm(train_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "\n",
    "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    val_iter = Hdf5Dataset(pl.Path(folder)/valid_filename,num_entries=N_SAMPLES)\n",
    "    val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    for src, tgt in tqdm(val_dataloader):\n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        tgt_input = tgt[:-1, :]\n",
    "\n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
    "        with torch.no_grad():\n",
    "            logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        \n",
    "        tgt_out = tgt[1:, :]\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "    return losses / len(val_dataloader)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"4\"></a>\n",
    "# 4. Training\n",
    "Now we have all the ingredients to train our model:\n",
    "The training is performed on 10 epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6250 [00:39<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [18]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, NUM_EPOCHS\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m      8\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[1;32m----> 9\u001B[0m     train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     10\u001B[0m     end_time \u001B[38;5;241m=\u001B[39m timer()\n\u001B[0;32m     11\u001B[0m     val_loss \u001B[38;5;241m=\u001B[39m evaluate(transformer)\n",
      "Input \u001B[1;32mIn [13]\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[1;34m(model, optimizer)\u001B[0m\n\u001B[0;32m      7\u001B[0m train_iter \u001B[38;5;241m=\u001B[39m Hdf5Dataset(pl\u001B[38;5;241m.\u001B[39mPath(folder)\u001B[38;5;241m/\u001B[39mtrain_filename,num_entries\u001B[38;5;241m=\u001B[39mN_SAMPLES,randomized\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      8\u001B[0m train_dataloader \u001B[38;5;241m=\u001B[39m DataLoader(train_iter, batch_size\u001B[38;5;241m=\u001B[39mBATCH_SIZE, collate_fn\u001B[38;5;241m=\u001B[39mcollate_fn)\n\u001B[1;32m---> 10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m src, tgt \u001B[38;5;129;01min\u001B[39;00m tqdm(train_dataloader):\n\u001B[0;32m     11\u001B[0m     src \u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39mto(DEVICE)\n\u001B[0;32m     12\u001B[0m     tgt \u001B[38;5;241m=\u001B[39m tgt\u001B[38;5;241m.\u001B[39mto(DEVICE)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[0;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[0;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[0;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[1;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    569\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 570\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    571\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    572\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n\u001B[1;32m---> 52\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollate_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mcollate_fn\u001B[1;34m(batch)\u001B[0m\n\u001B[0;32m     31\u001B[0m src_batch, tgt_batch \u001B[38;5;241m=\u001B[39m [], []\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m src_sample, tgt_sample \u001B[38;5;129;01min\u001B[39;00m batch:\n\u001B[1;32m---> 33\u001B[0m     src_batch\u001B[38;5;241m.\u001B[39mappend(\u001B[43mtext_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_sample\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrstrip\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;130;43;01m\\n\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     34\u001B[0m     tgt_batch\u001B[38;5;241m.\u001B[39mappend(text_transform(tgt_sample\u001B[38;5;241m.\u001B[39mrstrip(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)))\n\u001B[0;32m     36\u001B[0m src_batch \u001B[38;5;241m=\u001B[39m pad_sequence(src_batch, padding_value\u001B[38;5;241m=\u001B[39mPAD_IDX)\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36msequential_transforms.<locals>.func\u001B[1;34m(txt_input)\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfunc\u001B[39m(txt_input):\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m transform \u001B[38;5;129;01min\u001B[39;00m transforms:\n\u001B[1;32m----> 7\u001B[0m         txt_input \u001B[38;5;241m=\u001B[39m \u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtxt_input\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m txt_input\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mtensor_transform\u001B[1;34m(token_ids)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(\u001B[38;5;28mlen\u001B[39m(token_ids) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m5000\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m     18\u001B[0m     token_ids \u001B[38;5;241m=\u001B[39m token_ids[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m4998\u001B[39m]\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mcat((torch\u001B[38;5;241m.\u001B[39mtensor([BOS_IDX]),\n\u001B[0;32m     20\u001B[0m                   torch\u001B[38;5;241m.\u001B[39mtensor(token_ids),\n\u001B[0;32m     21\u001B[0m                   torch\u001B[38;5;241m.\u001B[39mtensor([EOS_IDX])))\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mtensor_transform\u001B[1;34m(token_ids)\u001B[0m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m(\u001B[38;5;28mlen\u001B[39m(token_ids) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m5000\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m     18\u001B[0m     token_ids \u001B[38;5;241m=\u001B[39m token_ids[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m4998\u001B[39m]\n\u001B[1;32m---> 19\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241m.\u001B[39mcat((torch\u001B[38;5;241m.\u001B[39mtensor([BOS_IDX]),\n\u001B[0;32m     20\u001B[0m                   torch\u001B[38;5;241m.\u001B[39mtensor(token_ids),\n\u001B[0;32m     21\u001B[0m                   torch\u001B[38;5;241m.\u001B[39mtensor([EOS_IDX])))\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1095\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_310_64.pyx:1053\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_310_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1.2\\plugins\\python-ce\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1.2\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1155\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1152\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1154\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1155\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\DataSpell 2022.1.2\\plugins\\python-ce\\helpers\\pydev\\pydevd.py:1170\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1167\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1169\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1170\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1172\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1174\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train_epoch(transformer, optimizer)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(transformer)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': transformer.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': val_loss,\n",
    "    }, pl.Path(checkpoint_folder)/model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[1;32mIn [37]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m _ \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mplot(\u001B[43mtrain_losses\u001B[49m)\n\u001B[0;32m      2\u001B[0m _ \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39mplot(val_losses)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "_ = plt.plot(train_losses)\n",
    "_ = plt.plot(val_losses)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5. Evaluation\n",
    "<a name=\"5\"></a>We can evaluate the produced correction using the ``greedy_decode`` function.\n",
    "In this function, at each timestep the most probable token is selected as correction at each time step, without evaluating its impact on the rest of the sequence.\n",
    "A better way could be to use a ``Beam Search``, choosing $k$ tokens at each timestep and then scoring each node with its log probability."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# function to generate output sequence using greedy algorithm\n",
    "# input: (input_length,batch_size)\n",
    "# output: (input_length)\n",
    "def greedy_decode(model, src, src_mask, src_padding_mask, max_len):\n",
    "    batch_size = src.shape[1]\n",
    "\n",
    "    src = src.to(DEVICE)\n",
    "    src_mask = src_mask.to(DEVICE)\n",
    "    src_padding_mask = src_padding_mask.to(DEVICE)\n",
    "    unfinished_sequences = torch.ones(1,batch_size).to(DEVICE)\n",
    "\n",
    "    context = transformer.encode(src, src_mask,src_padding_mask).to(DEVICE)\n",
    "    ys = torch.ones(1, batch_size).fill_(BOS_IDX).type(torch.long).to(DEVICE)\n",
    "    for i in range(max_len-1):\n",
    "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
    "                    .type(torch.bool)).to(DEVICE)\n",
    "        out = model.decode(ys, context, tgt_mask)\n",
    "        out = out.transpose(0, 1)\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob,dim=1)\n",
    "        #Predict only PAD_IDX after EOS_IDX is predicted\n",
    "        next_word = next_word * unfinished_sequences + (1-unfinished_sequences) * PAD_IDX\n",
    "        ys = torch.cat([ys,\n",
    "                        next_word], dim=0)\n",
    "        # if eos_token was found in one sentence, set sentence to finished\n",
    "\n",
    "        unfinished_sequences = unfinished_sequences.mul((next_word != EOS_IDX).long())\n",
    "        #\n",
    "        # stop when each sentence is finished, or if we exceed the maximum length\n",
    "        if unfinished_sequences.max() == 0:\n",
    "            break\n",
    "    return ys.int()\n",
    "\n",
    "\n",
    "# actual function to correct input sentence\n",
    "def correct(src_sentence: str, model: torch.nn.Module):\n",
    "    model.eval()\n",
    "    src = text_transform(src_sentence).view(-1, 1)\n",
    "    num_tokens = src.shape[0]\n",
    "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
    "    src_padding_mask = (torch.zeros(1, num_tokens)).type(torch.bool)\n",
    "    tgt_tokens = greedy_decode(\n",
    "        model,  src, src_mask,src_padding_mask, max_len=num_tokens + 5).flatten()\n",
    "    return ' '.join([vocab_transform.vocab.itos_[i] for i in tgt_tokens if i not in [PAD_IDX,BOS_IDX,EOS_IDX]])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \" And what started as a buzz turned into a fear. \"\n",
      "target: \" What started as a buzz has turned into a roar. \"\n",
      "prediction: \"and what started as a problem turned into a fear .\"\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(pl.Path(checkpoint_folder)/model_name)\n",
    "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "transformer.eval()\n",
    "\n",
    "# Pick one in 18M examples\n",
    "test_iter = Hdf5Dataset(pl.Path(folder)/test_filename,num_entries=None)\n",
    "\n",
    "src,trg = random.choice(test_iter)\n",
    "\n",
    "print(\"input: \\\"\",src,\"\\\"\")\n",
    "print(\"target: \\\"\",trg,\"\\\"\")\n",
    "\n",
    "prediction = correct(src,transformer)\n",
    "\n",
    "print(\"prediction: \\\"%s\\\"\"%prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from nltk.translate import chrf_score,bleu_score\n",
    "import re\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(tgt_sample.rstrip(\"\\n\"))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "def test(model,criterion):\n",
    "    model.eval()\n",
    "    pt = 0\n",
    "    rt = 0\n",
    "    ft = 0\n",
    "    tpt = 0\n",
    "\n",
    "    test_iter = Hdf5Dataset(pl.Path(folder)/test_filename,num_entries=10000)\n",
    "    test_dataloader = DataLoader(test_iter, batch_size=64,collate_fn=test_collate_fn)\n",
    "    for src, tgt in tqdm(test_dataloader):\n",
    "        src_mask, _, src_padding_mask, _ = create_mask(src, src)\n",
    "        num_tokens = src.shape[0]\n",
    "        with torch.no_grad():\n",
    "            pred_tokens = greedy_decode(\n",
    "                model,  src, src_mask,src_padding_mask, max_len=num_tokens + 5).T.flatten()\n",
    "            pred_sentences = ' '.join([vocab_transform.vocab.itos_[i] for i in pred_tokens if i not in [PAD_IDX,BOS_IDX,EOS_IDX]])\n",
    "            pred_sentences = re.sub(r'\\s([?.!,\"](?:\\s|$))', r'\\1', pred_sentences)\n",
    "        p,r,f,tp = chrf_score.chrf_precision_recall_fscore_support(pred_sentences,' '.join(tgt),n=3,beta=0.5)\n",
    "        pt = pt+p\n",
    "        rt = rt + r\n",
    "        f = ft + f\n",
    "        tpt = tpt + tp\n",
    "\n",
    "    pt = pt / len(test_dataloader)\n",
    "    rt = pt / len(test_dataloader)\n",
    "    ft = ft / len(test_dataloader)\n",
    "    tpt = tpt / len(test_dataloader)\n",
    "    return pt,rt,ft,tpt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 48/157 [02:12<04:59,  2.75s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [111]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m chrf \u001B[38;5;241m=\u001B[39m chrf_score\u001B[38;5;241m.\u001B[39mchrf_precision_recall_fscore_support\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m# print(chrf_score.chrf_precision_recall_fscore_support('Thinking of hiring The Pass Street Food Cafe for your event?','thinking of hiring the pass street food cafe for your event',n=1))\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m test_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtransformer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mchrf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(test_loss)\n",
      "Input \u001B[1;32mIn [110]\u001B[0m, in \u001B[0;36mtest\u001B[1;34m(model, criterion)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     27\u001B[0m     pred_tokens \u001B[38;5;241m=\u001B[39m greedy_decode(\n\u001B[0;32m     28\u001B[0m         model,  src, src_mask,src_padding_mask, max_len\u001B[38;5;241m=\u001B[39mnum_tokens \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m---> 29\u001B[0m     pred_sentences \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([vocab_transform\u001B[38;5;241m.\u001B[39mvocab\u001B[38;5;241m.\u001B[39mitos_[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m pred_tokens \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m [PAD_IDX,BOS_IDX,EOS_IDX]])\n\u001B[0;32m     30\u001B[0m     pred_sentences \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms([?.!,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m](?:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms|$))\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m, pred_sentences)\n\u001B[0;32m     31\u001B[0m p,r,f,tp \u001B[38;5;241m=\u001B[39m chrf_score\u001B[38;5;241m.\u001B[39mchrf_precision_recall_fscore_support(pred_sentences,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(tgt),n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,beta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n",
      "Input \u001B[1;32mIn [110]\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     27\u001B[0m     pred_tokens \u001B[38;5;241m=\u001B[39m greedy_decode(\n\u001B[0;32m     28\u001B[0m         model,  src, src_mask,src_padding_mask, max_len\u001B[38;5;241m=\u001B[39mnum_tokens \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m5\u001B[39m)\u001B[38;5;241m.\u001B[39mT\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m---> 29\u001B[0m     pred_sentences \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([vocab_transform\u001B[38;5;241m.\u001B[39mvocab\u001B[38;5;241m.\u001B[39mitos_[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m pred_tokens \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mPAD_IDX\u001B[49m\u001B[43m,\u001B[49m\u001B[43mBOS_IDX\u001B[49m\u001B[43m,\u001B[49m\u001B[43mEOS_IDX\u001B[49m\u001B[43m]\u001B[49m])\n\u001B[0;32m     30\u001B[0m     pred_sentences \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms([?.!,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m](?:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms|$))\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124m1\u001B[39m\u001B[38;5;124m'\u001B[39m, pred_sentences)\n\u001B[0;32m     31\u001B[0m p,r,f,tp \u001B[38;5;241m=\u001B[39m chrf_score\u001B[38;5;241m.\u001B[39mchrf_precision_recall_fscore_support(pred_sentences,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(tgt),n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m,beta\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(pl.Path(checkpoint_folder)/model_name)\n",
    "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "transformer.eval()\n",
    "\n",
    "chrf = chrf_score.chrf_precision_recall_fscore_support\n",
    "\n",
    "# print(chrf_score.chrf_precision_recall_fscore_support('Thinking of hiring The Pass Street Food Cafe for your event?','thinking of hiring the pass street food cafe for your event',n=1))\n",
    "\n",
    "test_loss = test(transformer,chrf)\n",
    "print(test_loss)\n",
    "\n",
    "# bleu = bleu_score.sentence_bleu()\n",
    "#\n",
    "# test_loss = test(transformer,bleu)\n",
    "# print(test_loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"6\"></a>\n",
    "# 6. Results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"7\"></a>\n",
    "# 7. References\n",
    "\n",
    "1. [Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "2. [The annotated transformer](http://nlp.seas.harvard.edu/annotated-transformer)\n",
    "3. [Pytorch tutorial on NMT with transformers](https://pytorch.org/tutorials/beginner/translation_transformer.html)\n",
    "4. [Speech and Language Processing, Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/)\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}