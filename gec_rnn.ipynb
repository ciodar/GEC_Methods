{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "execution": {
     "iopub.status.busy": "2022-07-04T16:49:25.64338Z",
     "iopub.execute_input": "2022-07-04T16:49:25.643928Z",
     "iopub.status.idle": "2022-07-04T16:49:25.655697Z",
     "shell.execute_reply.started": "2022-07-04T16:49:25.643887Z",
     "shell.execute_reply": "2022-07-04T16:49:25.654674Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Grammar Error Correction with RNNs\n",
    "======================================================\n",
    "\n",
    "This notebook shows an encoder-decoder model for Grammar Error Correction on C4 200M dataset.\n",
    "The following notebook was based on [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078) and followed Pytorch's sequence to sequence NMT [tutorials](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) and [pytorch-seq2seq](https://github.com/bentrevett/pytorch-seq2seq)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Sourcing and Processing\n",
    "\n",
    "C4 200M dataset from Google Research is used in this notebook. You can find more information about the C4 200M dataset on GR's [BEA 2021 paper](https://aclanthology.org/2021.bea-1.4/).\n",
    "The already [processed dataset](https://huggingface.co/datasets/liweili/c4_200m) was extracted from Huggingface, then was transformed to HDF5 format for better manageability. The conversion process was based on this [notebook](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/mechanics/custom-data-loader-csv.ipynb).\n",
    "The final version of the dataset is uploaded on [Kaggle](https://www.kaggle.com/datasets/dariocioni/c4200m).\n",
    "\n",
    "A custom class ``Hdf5Dataset`` based on ``torch.utils.data.Dataset`` is developed, which yields a pair of source-target raw sentences.\n",
    "\n",
    "| source                                             | target                                                  |\n",
    "|----------------------------------------------------|---------------------------------------------------------|\n",
    "| Much many brands and sellers still in the market.  | Many brands and sellers still in the market.            |\n",
    "| She likes playing in park and come here every week | She likes playing in the park and comes here every week |"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib as pl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Hdf5Dataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading entries from HDF5 databases\"\"\"\n",
    "\n",
    "    def __init__(self, h5_path, transform=None,num_entries = None):\n",
    "\n",
    "        self.h5f = h5py.File(h5_path, 'r')\n",
    "        if num_entries:\n",
    "            self.num_entries = num_entries\n",
    "        else:\n",
    "            self.num_entries = self.h5f['labels'].shape[0]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index > self.num_entries:\n",
    "            raise StopIteration\n",
    "        input = self.h5f['input'][index].decode('utf-8')\n",
    "        label = self.h5f['labels'][index].decode('utf-8')\n",
    "        if self.transform is not None:\n",
    "            features = self.transform(input)\n",
    "        return input, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_entries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from typing import Iterable, List\n",
    "from tqdm import tqdm\n",
    "import pathlib as pl\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# helper function to yield list of tokens\n",
    "def yield_tokens(data_iter: Iterable, index: int) -> List[str]:\n",
    "    language_index = {SRC_LANGUAGE: 0, TGT_LANGUAGE: 1}\n",
    "    for data_sample in tqdm(data_iter):\n",
    "        if data_sample[index] and isinstance(data_sample[index],str):\n",
    "            yield token_transform(data_sample[index])\n",
    "\n",
    "SRC_LANGUAGE = 'incorrect'\n",
    "TGT_LANGUAGE = 'correct'\n",
    "# MAX_LENGTH = 512\n",
    "VOCAB_SIZE = 20000\n",
    "N_SAMPLES = 1000\n",
    "\n",
    "# # Define special symbols and indices\n",
    "UNK_IDX,PAD_IDX, BOS_IDX, EOS_IDX = 0,1,2,3\n",
    "# # Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<pad>','<unk>', '[CLS]', '[SEP]']\n",
    "\n",
    "# Place-holders\n",
    "token_transform =get_tokenizer('basic_english')\n",
    "vocab_transform = None\n",
    "\n",
    "folder = 'D:\\Datasets\\c4_200m\\data\\hdf5'\n",
    "train_filename = 'C4_200M.hf5-00000-of-00010'\n",
    "valid_filename = 'C4_200M.hf5-00001-of-00010'\n",
    "embedding_path = 'D:\\Datasets\\glove\\glove.42B.300d.txt'\n",
    "checkpoint_folder = 'D:\\Datasets\\c4_200m\\checkpoints'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizing and Embedding\n",
    "Data is then tokenized by the standard tokenizer from ``torchtext`` library, which performs basic normalization and splitting by space. Normalization includes\n",
    "- lowercasing\n",
    "- complete some basic text normalization for English words as follows:\n",
    "    add spaces before and after '\\''\n",
    "    remove '\\\"',\n",
    "    add spaces before and after '.'\n",
    "    replace '<br \\/>'with single space\n",
    "    add spaces before and after ','\n",
    "    add spaces before and after '('\n",
    "    add spaces before and after ')'\n",
    "    add spaces before and after '!'\n",
    "    add spaces before and after '?'\n",
    "    replace ';' with single space\n",
    "    replace ':' with single space\n",
    "    replace multiple spaces with single space\n",
    "\n",
    "Possible future enhancements could be:\n",
    "- A tokenization library like ``spacy``\n",
    "- Using pretrained embeddings such as ``Word2vec`` or ``GloVe`` Embeddings, which was trained on Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<UNK>', '<PAD>', '<BOS>', '<EOS>']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "vocab_transform = torch.load('vocab/vocab_20K.pth')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Collation\n",
    "---------\n",
    "\n",
    "An iterator over ``Hdf5dataset`` yields a pair of raw strings.\n",
    "We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network.\n",
    "Below we define our collate function that convert batch of raw strings into batch tensors that can be fed directly into our model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# def glove_transform(tokens: List[str]):\n",
    "\n",
    "\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids: List[int]):\n",
    "    return torch.cat((torch.tensor([BOS_IDX]),\n",
    "                      torch.tensor(token_ids),\n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and tgt language text transforms to convert raw strings into tensors indices\n",
    "text_transform = sequential_transforms(token_transform,\n",
    "                                               vocab_transform,\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(text_transform(src_sample.rstrip(\"\\n\")))\n",
    "        tgt_batch.append(text_transform(tgt_sample.rstrip(\"\\n\")))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's finally see all the three steps of conversion of a sentence to an embedding tensor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized input:\n",
      " ['data', 'mining', 'is', 'awesome', '!']\n",
      "encoded input:\n",
      " [157, 1185, 13, 1480, 32]\n",
      "transformed input:\n",
      " tensor([   2,  157, 1185,   13, 1480,   32,    3])\n"
     ]
    }
   ],
   "source": [
    "text = 'data mining is awesome!'\n",
    "tokenized_input = token_transform(text)\n",
    "print(\"tokenized input:\\n\",tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(\"encoded input:\\n\",encoded_input)\n",
    "\n",
    "print(\"transformed input:\\n\",text_transform(text))\n",
    "\n",
    "\n",
    "\n",
    "# my_embedding_layer = torch.nn.Embedding.from_pretrained(torch.from_numpy(embs_npa).float())\n",
    "# assert my_embedding_layer.weight.shape == embs_npa.shape\n",
    "# embedding1 = my_embedding_layer(tensor_transform(encoded_input))\n",
    "# print(embedding1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Unknown words\n",
    "In this version, unknown words are all converted to <unk> and converted to the same embedding."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataminingisawesome', '!']\n",
      "[0, 32]\n"
     ]
    }
   ],
   "source": [
    "text = 'dataminingisawesome!'\n",
    "tokenized_input = token_transform(text)\n",
    "print(tokenized_input)\n",
    "\n",
    "encoded_input = vocab_transform(tokenized_input)\n",
    "print(encoded_input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RNN Network\n",
    "This network is a seq2seq network composed by GRU layers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "teacher_forcing_ratio = 0.5\n",
    "torch.manual_seed(0)\n",
    "\n",
    "EMB_SIZE = 300\n",
    "HIDDEN_SIZE = 512\n",
    "BATCH_SIZE = 16\n",
    "NUM_ENCODER_LAYERS = 1\n",
    "NUM_DECODER_LAYERS =1\n",
    "\n",
    "learning_rate = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "During training, we need a subsequent word mask that will prevent model to look into the future words when making predictions. We will also need masks to hide source and target padding tokens. Below, let's define a function that will take care of both."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_mask(src):\n",
    "    src_seq_len = src.shape[0]\n",
    "\n",
    "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "    return src_mask, src_padding_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg in tqdm(iterator):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src = src.to(DEVICE)\n",
    "        trg = trg.to(DEVICE)\n",
    "\n",
    "        output = model(src, trg)\n",
    "\n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "\n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for src, trg in tqdm(iterator):\n",
    "\n",
    "            src = src.to(DEVICE)\n",
    "            trg = trg.to(DEVICE)\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To wrap the encoder and decoder, we can define a ``Seq2Seq`` class where the forward passing is performed.\n",
    "Teacher forcing can be imposed and also be done with a varying ratio between 0 and 1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hid_dim == decoder.hid_dim,\"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "\n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        #last hidden state of the encoder is the context\n",
    "        context = self.encoder(src)\n",
    "\n",
    "        #context also used as the initial hidden state of the decoder\n",
    "        hidden = context\n",
    "\n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "\n",
    "            #insert input token embedding, previous hidden state and the context state\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, context)\n",
    "\n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "\n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "\n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's now define the parameters of our model and instantiate the same. Below, we also define our loss function which is the cross-entropy loss and the optmizer used for training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 41,787,040 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "from models.rnn_seq2seq import Encoder, Decoder\n",
    "\n",
    "# attn = Attention(HIDDEN_SIZE, HIDDEN_SIZE)\n",
    "\n",
    "encoder1 = Encoder(VOCAB_SIZE,EMB_SIZE,HIDDEN_SIZE,0)\n",
    "decoder1 = Decoder(VOCAB_SIZE,EMB_SIZE,HIDDEN_SIZE,0.1)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "#optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate , betas=(0.9, 0.98), eps=1e-9)\n",
    "#decoder_optimizer = torch.optim.Adam(encoder1.parameters(), lr = learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "model = Seq2Seq(encoder1,decoder1,DEVICE).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we have all the ingredients to train our model. Let's do it!\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:29<00:00,  2.13it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 9.970, Val loss: 9.973, Epoch time = 29.561s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.36it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 9.963, Val loss: 9.973, Epoch time = 26.750s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.35it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 9.968, Val loss: 9.973, Epoch time = 26.802s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.33it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 9.973, Val loss: 9.973, Epoch time = 26.990s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.37it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 9.970, Val loss: 9.973, Epoch time = 26.593s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.38it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 9.970, Val loss: 9.973, Epoch time = 26.525s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.37it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 9.970, Val loss: 9.973, Epoch time = 26.531s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.38it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 9.966, Val loss: 9.973, Epoch time = 26.497s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.37it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 9.970, Val loss: 9.973, Epoch time = 26.552s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:26<00:00,  2.38it/s]\n",
      "100%|██████████| 63/63 [00:08<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 9.971, Val loss: 9.973, Epoch time = 26.529s\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from timeit import default_timer as timer\n",
    "NUM_EPOCHS = 10\n",
    "CLIP = 1\n",
    "RESUME = False\n",
    "\n",
    "train_iter = Hdf5Dataset(pl.Path(folder)/train_filename,num_entries=N_SAMPLES)\n",
    "train_dataloader = DataLoader(train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "val_iter = Hdf5Dataset(pl.Path(folder)/valid_filename,num_entries=N_SAMPLES)\n",
    "val_dataloader = DataLoader(val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "model.train()\n",
    "if RESUME:\n",
    "    checkpoint = torch.load(pl.Path('checkpoints')/\"model.pt\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    start_time = timer()\n",
    "    train_loss = train(model,train_dataloader,optimizer,loss_fn,0)\n",
    "    end_time = timer()\n",
    "    val_loss = evaluate(model,val_dataloader,loss_fn)\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': val_loss,\n",
    "    }, pl.Path('checkpoints')/\"model.pt\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To evaluate the results, we can define a function which gets in input a text (already converted in a tensor with the previously defined collation pipeline) and returns the output sequence, already converted by taking the highest probability token at each time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import re\n",
    "# function to generate output sequence using greedy algorithm \n",
    "def correct_sentence_vectorized(src_tensor, model, max_len=50):\n",
    "    assert isinstance(src_tensor, torch.Tensor)\n",
    "\n",
    "    model.eval()\n",
    "    src_tensor = src_tensor.unsqueeze(1).to(DEVICE)\n",
    "\n",
    "    trg_vocab_size = model.decoder.output_dim\n",
    "\n",
    "    #tensor to store decoder outputs\n",
    "    outputs = torch.zeros(max_len, 1, trg_vocab_size).to(DEVICE)\n",
    "\n",
    "    #last hidden state of the encoder is the context\n",
    "    with torch.no_grad():\n",
    "        context = model.encoder(src_tensor)\n",
    "\n",
    "    #context also used as the initial hidden state of the decoder\n",
    "    hidden = context\n",
    "\n",
    "    #first input to the decoder is the <sos> tokens\n",
    "    input = src_tensor[0,:]\n",
    "    # enc_src = [batch_sz, src_len, hid_dim]\n",
    "    # Even though some examples might have been completed by producing a <eos> token\n",
    "    # we still need to feed them through the model because other are not yet finished\n",
    "    # and all examples act as a batch. Once every single sentence prediction encounters\n",
    "    # <eos> token, then we can stop predicting.\n",
    "    for t in range(1, max_len):\n",
    "\n",
    "        #insert input token embedding, previous hidden state and the context state\n",
    "        #receive output tensor (predictions) and new hidden state\n",
    "        output, hidden = model.decoder(input, hidden, context)\n",
    "\n",
    "        #place predictions in a tensor holding predictions for each token\n",
    "        outputs[t] = output\n",
    "\n",
    "        #get the highest predicted token from our predictions\n",
    "        top1 = output.argmax(1)\n",
    "\n",
    "        #if teacher forcing, use actual next token as next input\n",
    "        #if not, use predicted token\n",
    "        input = top1\n",
    "\n",
    "    pred_sentence = []\n",
    "\n",
    "    for i in range(1, len(outputs)):\n",
    "        topv, topi = outputs[i,:,:].topk(1)\n",
    "        pred_sentence.append(vocab_transform.vocab.itos_[topi])\n",
    "        if topi == EOS_IDX:\n",
    "            break\n",
    "\n",
    "    return ' '.join(pred_sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: \" Bobby was despentate to escape. \"\n",
      "target: \" Bobby was desperate to escape. \"\n",
      "demand analysts edwin acidity mayonnaise driving 122 submission myers destiny furnishing steroids commands breakers whatsapp gibson way newer attracts vanities methods communities meltdown telstra western spice methane pulled hurry hosts clever lam puerto paste ink dropbox thou hashtags equipment bits cambridge chandler alec evenings wolf bsc 2006 binder leaching\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(pl.Path('checkpoints')/\"model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Pick one in 18M examples\n",
    "val_iter = Hdf5Dataset(pl.Path(folder)/valid_filename,num_entries=None)\n",
    "\n",
    "src,trg = random.choice(val_iter)\n",
    "\n",
    "print(\"input: \\\"\",src,\"\\\"\")\n",
    "print(\"target: \\\"\",trg,\"\\\"\")\n",
    "\n",
    "src = text_transform(src)\n",
    "\n",
    "print(correct_sentence_vectorized(src,model))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "References\n",
    "----------\n",
    "\n",
    "1. Attention is all you need paper.\n",
    "   https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "2. The annotated transformer. https://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding\n",
    "\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}